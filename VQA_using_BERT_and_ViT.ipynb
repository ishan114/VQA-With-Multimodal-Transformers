{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "VQA using BERT and ViT ",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishan114/VQA-With-Multimodal-Transformers/blob/main/VQA_using_BERT_and_ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'processed-daquar-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1876338%2F3064985%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240315%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240315T104325Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D779e1f9d87010633713173b2976ba76d5030a0e1476a994153c8be362f165561ccda482e524152f708c4ffae355c0aae0daeab1863815833d332d232a32e200c4eab1f45d9b13a05b4c640ecaab1d5727125dda63a39a5d383778361203e434e5c96a3adf8f023296c9fbaa22eadfd6507af4df01ecaef4e6a1efc6210c0eb8e6313fc7c2974817842284ee567c7454b481a0855ced6b6c49d2a768823f73c863c5b15cef90933e4985f45c9a919334af74b6ba89687d43d5d54da7549feb98e2c707da3b5e891d65256f7b854e99f656e24e4db5e080f9ca66954ba54a1f9a3b27d0849b170a3e489a8c999e9adffb1f207249e079f82d28e1667a6b3922270'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ER9zpiU30w11"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Description\n",
        "\n",
        "The DAQUAR dataset, a significant Visual Question Answering (VQA) dataset, was the first of its kind. It comprises 6794 training and 5674 test question-answer pairs, derived from images in the NYU-Depth V2 Dataset, resulting in an average of 9 pairs per image. The dataset is a refined version of the Full DAQUAR Dataset, featuring normalized questions for easier processing by tokenizers. Image IDs, questions, and answers are organized in a tabular (CSV) format, facilitating seamless use for training VQA models.\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "O_aNi05F0w13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!pip install -U datasets\n",
        "!pip install transformers==4.18.0 nltk==3.2.4 numpy==1.21.6 datasets==2.1.0 pandas==1.3.5\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:46:42.673533Z",
          "iopub.execute_input": "2023-11-11T19:46:42.673944Z",
          "iopub.status.idle": "2023-11-11T19:47:26.898151Z",
          "shell.execute_reply.started": "2023-11-11T19:46:42.673912Z",
          "shell.execute_reply": "2023-11-11T19:47:26.896663Z"
        },
        "trusted": true,
        "id": "Zi-pU25D0w14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from datasets import load_dataset, set_caching_enabled\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    # Preprocessing / Common\n",
        "    AutoTokenizer, AutoFeatureExtractor,\n",
        "    # Text & Image Models (Now, image transformers like ViTModel, DeiTModel, BEiT can also be loaded using AutoModel)\n",
        "    AutoModel, AutoConfig,\n",
        "    # Training / Evaluation\n",
        "    TrainingArguments, Trainer,\n",
        "    # Misc\n",
        "    logging\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "import subprocess\n",
        "\n",
        "# Download and unzip wordnet\n",
        "try:\n",
        "    nltk.data.find('wordnet.zip')\n",
        "except:\n",
        "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
        "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
        "    subprocess.run(command.split())\n",
        "    nltk.data.path.append('/kaggle/working/')\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:47:26.90048Z",
          "iopub.execute_input": "2023-11-11T19:47:26.900835Z",
          "iopub.status.idle": "2023-11-11T19:47:46.815482Z",
          "shell.execute_reply.started": "2023-11-11T19:47:26.900804Z",
          "shell.execute_reply": "2023-11-11T19:47:46.814363Z"
        },
        "trusted": true,
        "id": "VUjeqkf40w14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cache for Hugging Face Transformers and Datasets.\n",
        "os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n",
        "\n",
        "set_caching_enabled(True)\n",
        "logging.set_verbosity_error()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:47:46.816997Z",
          "iopub.execute_input": "2023-11-11T19:47:46.817701Z",
          "iopub.status.idle": "2023-11-11T19:47:46.823718Z",
          "shell.execute_reply.started": "2023-11-11T19:47:46.817665Z",
          "shell.execute_reply": "2023-11-11T19:47:46.822501Z"
        },
        "trusted": true,
        "id": "PS9vbhVP0w15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "#Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:47:46.827011Z",
          "iopub.execute_input": "2023-11-11T19:47:46.827714Z",
          "iopub.status.idle": "2023-11-11T19:47:46.900141Z",
          "shell.execute_reply.started": "2023-11-11T19:47:46.827675Z",
          "shell.execute_reply": "2023-11-11T19:47:46.899015Z"
        },
        "trusted": true,
        "id": "G1Ay7G290w15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "id": "7QrqEBb20w15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset from CSV files for training and testing\n",
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files={\n",
        "        \"train\": \"/kaggle/input/processed-daquar-dataset/data_train.csv\",\n",
        "        \"test\": \"/kaggle/input/processed-daquar-dataset/data_eval.csv\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# read answer space from file and split into an array by line\n",
        "with open(\"/kaggle/input/processed-daquar-dataset/answer_space.txt\") as f:\n",
        "    answer_space = f.read().splitlines()\n",
        "\n",
        "# label each item in the dataset with their respective answers\n",
        "dataset = dataset.map(\n",
        "    lambda examples: {\n",
        "        'label': [\n",
        "            answer_space.index(ans.replace(\" \", \"\").split(\",\")[0])  # select the 1st answer if multiple answers are provided\n",
        "            for ans in examples['answer']\n",
        "        ]\n",
        "    },\n",
        "    batched=True\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:47:46.901461Z",
          "iopub.execute_input": "2023-11-11T19:47:46.90177Z",
          "iopub.status.idle": "2023-11-11T19:47:47.435043Z",
          "shell.execute_reply.started": "2023-11-11T19:47:46.901745Z",
          "shell.execute_reply": "2023-11-11T19:47:47.434099Z"
        },
        "trusted": true,
        "id": "hmgCcpLM0w15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's view some data"
      ],
      "metadata": {
        "id": "FU7n5RUK0w15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as display\n",
        "\n",
        "def show_example(train=True, idx=None):\n",
        "    dataset_type = \"train\" if train else \"test\"\n",
        "    data = dataset[dataset_type]\n",
        "\n",
        "    if idx is None:\n",
        "        idx = np.random.randint(len(data))\n",
        "\n",
        "    image_path =  \"/kaggle/input/processed-daquar-dataset/images/\"+ f\"{data[idx]['image_id']}.png\"\n",
        "    image = Image.open(image_path)\n",
        "    display.display(image)\n",
        "\n",
        "    question = data[idx][\"question\"]\n",
        "    answer = data[idx][\"answer\"]\n",
        "    label = data[idx][\"label\"]\n",
        "\n",
        "    print(f\"Question:\\t {question}\")\n",
        "    print(f\"Answer:\\t\\t {answer} (Label: {label})\")\n",
        "\n",
        "    return answer\n",
        "show_example()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:47:47.436533Z",
          "iopub.execute_input": "2023-11-11T19:47:47.436878Z",
          "iopub.status.idle": "2023-11-11T19:47:47.671557Z",
          "shell.execute_reply.started": "2023-11-11T19:47:47.436848Z",
          "shell.execute_reply": "2023-11-11T19:47:47.670632Z"
        },
        "trusted": true,
        "id": "TQwMAokR0w16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a Multimodal Collator for the Dataset\n",
        "\n",
        "This collator is designed to be utilized within the Trainer() to seamlessly construct the Dataloader from the dataset, streamlining the input pipeline to the model.\n",
        "\n",
        "The collator's primary function is to handle both textual (question) and image data. It processes the question text, tokenizing it and generating attention masks. Simultaneously, it featurizes the image, essentially encapsulating its pixel values. These processed inputs are then fed into our multimodal transformer model, facilitating the question-answering process."
      ],
      "metadata": {
        "id": "r6501J-X0w16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class MultimodalCollator:\n",
        "    tokenizer: AutoTokenizer\n",
        "    preprocessor: AutoFeatureExtractor\n",
        "\n",
        "    def tokenize_text(self, texts: List[str]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Tokenize text inputs and return relevant tokenized information.\n",
        "        \"\"\"\n",
        "        encoded_text = self.tokenizer(\n",
        "            text=texts,\n",
        "            padding='longest',\n",
        "            max_length=24,\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
        "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
        "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
        "        }\n",
        "\n",
        "    def preprocess_images(self, images: List[str]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Extract features from images and return the processed pixel values.\n",
        "        \"\"\"\n",
        "        processed_images = self.preprocessor(\n",
        "            images=[\n",
        "                Image.open(os.path.join(\"/kaggle/input/processed-daquar-dataset/images/\", f\"{image_id}.png\")).convert('RGB')\n",
        "                for image_id in images\n",
        "            ],\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
        "        }\n",
        "\n",
        "    def __call__(self, raw_batch_dict) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Process raw batch data, tokenize text and extract image features, returning a dictionary\n",
        "        containing processed inputs and labels.\n",
        "        \"\"\"\n",
        "        question_batch = raw_batch_dict['question'] if isinstance(raw_batch_dict, dict) else [i['question'] for i in raw_batch_dict]\n",
        "        image_id_batch = raw_batch_dict['image_id'] if isinstance(raw_batch_dict, dict) else [i['image_id'] for i in raw_batch_dict]\n",
        "        label_batch = raw_batch_dict['label'] if isinstance(raw_batch_dict, dict) else [i['label'] for i in raw_batch_dict]\n",
        "\n",
        "        return {\n",
        "            **self.tokenize_text(question_batch),\n",
        "            **self.preprocess_images(image_id_batch),\n",
        "            'labels': torch.tensor(label_batch, dtype=torch.int64),\n",
        "        }\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:47:47.673225Z",
          "iopub.execute_input": "2023-11-11T19:47:47.673569Z",
          "iopub.status.idle": "2023-11-11T19:47:47.687861Z",
          "shell.execute_reply.started": "2023-11-11T19:47:47.673541Z",
          "shell.execute_reply": "2023-11-11T19:47:47.686759Z"
        },
        "trusted": true,
        "id": "YP_HEv3n0w16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Multimodal VQA Model Architecture\n",
        "\n",
        "Multimodal models can be of various forms to capture information from the text & image modalities, along with some cross-modal interaction as well. Here, we explore \"Fusion\" Models, that fuse information from the text encoder & image encoder to perform the downstream task (visual question answering).\n",
        "\n",
        "The text encoder can be a text-based transformer model (like BERT, RoBERTa, etc.) while the image encoder could be an image transformer (like ViT, Deit, BeIT, etc.). After passing the tokenized question through the text-based transformer & the image features through the image transformer, the outputs are concatenated & passed through a fully-connected network with an output having the same dimensions as the answer-space.\n",
        "\n",
        "Since we model the VQA task as a multi-class classification, it is natural to use the Cross-Entropy Loss as the loss function."
      ],
      "metadata": {
        "id": "VXpmD4Wt0w16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalVQAModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_labels: int = len(answer_space),\n",
        "        intermediate_dim: int = 512,\n",
        "        pretrained_text_name: str = 'bert-base-uncased',\n",
        "        pretrained_image_name: str = 'google/vit-base-patch16-224-in21k'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the Multimodal VQA Model.\n",
        "\n",
        "        Args:\n",
        "            num_labels (int): Number of labels in the answer space.\n",
        "            intermediate_dim (int): Dimensionality of the intermediate layer in the fusion module.\n",
        "            pretrained_text_name (str): Pretrained name for the text encoder.\n",
        "            pretrained_image_name (str): Pretrained name for the image encoder.\n",
        "        \"\"\"\n",
        "        super(MultimodalVQAModel, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.pretrained_text_name = pretrained_text_name\n",
        "        self.pretrained_image_name = pretrained_image_name\n",
        "\n",
        "        # Text and image encoders\n",
        "\n",
        "        self.text_encoder = AutoModel.from_pretrained(self.pretrained_text_name)\n",
        "        self.image_encoder = AutoModel.from_pretrained(self.pretrained_image_name)\n",
        "\n",
        "        # Fusion module\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
        "\n",
        "        # Loss function\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: torch.LongTensor,\n",
        "        pixel_values: torch.FloatTensor,\n",
        "        attention_mask: Optional[torch.LongTensor] = None,\n",
        "        token_type_ids: Optional[torch.LongTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.LongTensor): Tokenized input IDs for text.\n",
        "            pixel_values (torch.FloatTensor): Pixel values for images.\n",
        "            attention_mask (Optional[torch.LongTensor]): Attention mask for text.\n",
        "            token_type_ids (Optional[torch.LongTensor]): Token type IDs for text.\n",
        "            labels (Optional[torch.LongTensor]): Ground truth labels.\n",
        "\n",
        "        Returns:\n",
        "            Dict: Dictionary containing model outputs, including logits. If labels are provided, also includes loss.\n",
        "        \"\"\"\n",
        "        # Encode text with masking\n",
        "        encoded_text = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        # Encode images\n",
        "        encoded_image = self.image_encoder(\n",
        "            pixel_values=pixel_values,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        # Combine encoded texts and images\n",
        "        fused_output = self.fusion(\n",
        "            torch.cat(\n",
        "                [\n",
        "                    encoded_text['pooler_output'],\n",
        "                    encoded_image['pooler_output'],\n",
        "                ],\n",
        "                dim=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Make predictions\n",
        "        logits = self.classifier(fused_output)\n",
        "\n",
        "        out = {\"logits\": logits}\n",
        "        if labels is not None:\n",
        "            loss = self.criterion(logits, labels)\n",
        "            out[\"loss\"] = loss\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:47:47.689189Z",
          "iopub.execute_input": "2023-11-11T19:47:47.689568Z",
          "iopub.status.idle": "2023-11-11T19:47:47.708674Z",
          "shell.execute_reply.started": "2023-11-11T19:47:47.689541Z",
          "shell.execute_reply": "2023-11-11T19:47:47.707564Z"
        },
        "trusted": true,
        "id": "LYX9jp2i0w17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multimodal_vqa_collator_and_model(text_encoder='bert-base-uncased', image_encoder='google/vit-base-patch16-224-in21k'):\n",
        "    \"\"\"\n",
        "    Creates a Multimodal VQA collator and model.\n",
        "\n",
        "    Args:\n",
        "        text_encoder (str): Pretrained name for the text encoder.\n",
        "        image_encoder (str): Pretrained name for the image encoder.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: Multimodal collator and VQA model.\n",
        "    \"\"\"\n",
        "    # Initialize tokenizer and feature extractor\n",
        "    tokenizer = AutoTokenizer.from_pretrained(text_encoder)\n",
        "    preprocessor = AutoFeatureExtractor.from_pretrained(image_encoder)\n",
        "\n",
        "    # Create Multimodal Collator\n",
        "    multimodal_collator = MultimodalCollator(\n",
        "        tokenizer=tokenizer,\n",
        "        preprocessor=preprocessor,\n",
        "    )\n",
        "\n",
        "    # Create Multimodal VQA Model\n",
        "    multimodal_model = MultimodalVQAModel(\n",
        "        pretrained_text_name=text_encoder,\n",
        "        pretrained_image_name=image_encoder\n",
        "    ).to(device)\n",
        "\n",
        "    return multimodal_collator, multimodal_model\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:47:47.710349Z",
          "iopub.execute_input": "2023-11-11T19:47:47.710757Z",
          "iopub.status.idle": "2023-11-11T19:47:47.724555Z",
          "shell.execute_reply.started": "2023-11-11T19:47:47.710728Z",
          "shell.execute_reply": "2023-11-11T19:47:47.72345Z"
        },
        "trusted": true,
        "id": "52M6Y5dv0w17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Metrics in Visual Question Answering: Wu & Palmer Similarity\n",
        "\n",
        "The Wu & Palmer similarity is a metric designed to measure the semantic similarity between two words or phrases. It does so by considering the positions of the concepts (c1 and c2) in a taxonomy and their relative location to their Least Common Subsumer (LCS(c1, c2)).\n",
        "\n",
        "In the context of a directed acyclic graph, the Least Common Subsumer is the deepest node that has both considered nodes as descendants. Importantly, each node is considered a descendant of itself.\n",
        "\n",
        "Wu & Palmer similarity proves effective for single-word answers (the primary focus in our task), but it may not be suitable for phrases or sentences due to its design.\n",
        "\n",
        "The Natural Language Toolkit (nltk) provides an implementation of the Wu & Palmer similarity score based on the WordNet taxonomy. This implementation to align with the definition of Wu & Palmer similarity as specified in the DAQUAR dataset."
      ],
      "metadata": {
        "id": "rs7O0Wju0w17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wup_measure(a, b, similarity_threshold=0.925):\n",
        "    \"\"\"\n",
        "    Computes the Wu-Palmer similarity score between two words or phrases.\n",
        "\n",
        "    Args:\n",
        "        a (str): First word or phrase.\n",
        "        b (str): Second word or phrase.\n",
        "        similarity_threshold (float): Threshold for similarity to consider semantic fields.\n",
        "\n",
        "    Returns:\n",
        "        float: Wu-Palmer similarity score.\n",
        "    \"\"\"\n",
        "    def get_semantic_field(word):\n",
        "        \"\"\"\n",
        "        Retrieves the semantic field for a word.\n",
        "\n",
        "        Args:\n",
        "            word (str): Word to retrieve the semantic field for.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: Tuple containing the semantic field and weight.\n",
        "        \"\"\"\n",
        "        weight = 1.0\n",
        "        semantic_field = wordnet.synsets(word, pos=wordnet.NOUN)\n",
        "        return semantic_field, weight\n",
        "\n",
        "    def get_stem_word(word):\n",
        "        \"\"\"\n",
        "        Processes words in the form 'word\\d+:wordid' by returning the word and downweighting.\n",
        "\n",
        "        Args:\n",
        "            word (str): Word to process.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: Tuple containing the processed word and weight.\n",
        "        \"\"\"\n",
        "        weight = 1.0\n",
        "        return word, weight\n",
        "\n",
        "    global_weight = 1.0\n",
        "\n",
        "    # Get stem words and weights\n",
        "    a, global_weight_a = get_stem_word(a)\n",
        "    b, global_weight_b = get_stem_word(b)\n",
        "    global_weight = min(global_weight_a, global_weight_b)\n",
        "\n",
        "    # Check if words are the same\n",
        "    if a == b:\n",
        "        return 1.0 * global_weight\n",
        "\n",
        "    # Check for empty strings\n",
        "    if a == \"\" or b == \"\":\n",
        "        return 0\n",
        "\n",
        "    # Get semantic fields and weights\n",
        "    interp_a, weight_a = get_semantic_field(a)\n",
        "    interp_b, weight_b = get_semantic_field(b)\n",
        "\n",
        "    # Check for empty semantic fields\n",
        "    if interp_a == [] or interp_b == []:\n",
        "        return 0\n",
        "\n",
        "    # Find the most optimistic interpretation\n",
        "    global_max = 0.0\n",
        "    for x in interp_a:\n",
        "        for y in interp_b:\n",
        "            local_score = x.wup_similarity(y)\n",
        "            if local_score > global_max:\n",
        "                global_max = local_score\n",
        "\n",
        "    # Use semantic fields and downweight unless the score is high (indicating synonyms)\n",
        "    if global_max < similarity_threshold:\n",
        "        interp_weight = 0.1\n",
        "    else:\n",
        "        interp_weight = 1.0\n",
        "\n",
        "    final_score = global_max * weight_a * weight_b * interp_weight * global_weight\n",
        "    return final_score\n",
        "\n",
        "def batch_wup_measure(labels, preds):\n",
        "    \"\"\"\n",
        "    Computes the average Wu-Palmer similarity score for a batch of predicted and ground truth labels.\n",
        "\n",
        "    Args:\n",
        "        labels (List): List of ground truth labels.\n",
        "        preds (List): List of predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        float: Average Wu-Palmer similarity score for the batch.\n",
        "    \"\"\"\n",
        "    wup_scores = [wup_measure(answer_space[label], answer_space[pred]) for label, pred in zip(labels, preds)]\n",
        "    return np.mean(wup_scores)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:47:47.728779Z",
          "iopub.execute_input": "2023-11-11T19:47:47.729664Z",
          "iopub.status.idle": "2023-11-11T19:47:47.742905Z",
          "shell.execute_reply.started": "2023-11-11T19:47:47.729622Z",
          "shell.execute_reply": "2023-11-11T19:47:47.74186Z"
        },
        "trusted": true,
        "id": "pPcxNLjh0w17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.random.randint(len(answer_space), size=5)\n",
        "preds = np.random.randint(len(answer_space), size=5)\n",
        "\n",
        "def showAnswers(ids):\n",
        "    print([answer_space[id] for id in ids])\n",
        "\n",
        "showAnswers(labels)\n",
        "showAnswers(preds)\n",
        "\n",
        "print(\"Predictions vs Labels: \", batch_wup_measure(labels, preds))\n",
        "print(\"Labels vs Labels: \", batch_wup_measure(labels, labels))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:47:47.744254Z",
          "iopub.execute_input": "2023-11-11T19:47:47.744603Z",
          "iopub.status.idle": "2023-11-11T19:47:50.334869Z",
          "shell.execute_reply.started": "2023-11-11T19:47:47.744567Z",
          "shell.execute_reply": "2023-11-11T19:47:50.333638Z"
        },
        "trusted": true,
        "id": "eNhvLGQ00w18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Computes evaluation metrics for a given set of logits and labels.\n",
        "\n",
        "    Args:\n",
        "        eval_tuple (Tuple): Tuple containing logits and corresponding ground truth labels.\n",
        "\n",
        "    Returns:\n",
        "        Dict: Dictionary of computed metrics, including WUP similarity, accuracy, and F1 score.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_tuple\n",
        "\n",
        "    # Calculate predictions\n",
        "    preds = logits.argmax(axis=-1)\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = {\n",
        "        \"wups\": batch_wup_measure(labels, preds),\n",
        "        \"acc\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average='macro')\n",
        "    }\n",
        "\n",
        "    return metrics\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:47:50.336188Z",
          "iopub.execute_input": "2023-11-11T19:47:50.336525Z",
          "iopub.status.idle": "2023-11-11T19:47:50.344054Z",
          "shell.execute_reply.started": "2023-11-11T19:47:50.336498Z",
          "shell.execute_reply": "2023-11-11T19:47:50.343108Z"
        },
        "trusted": true,
        "id": "zxcwLbuL0w18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model and evaluate"
      ],
      "metadata": {
        "id": "ujkN9HMr0w18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments for the model\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/kaggle/working/checkpoint/\",            # Output directory for checkpoints and logs=\n",
        "    seed=12345,                         # Seed for reproducibility\n",
        "    evaluation_strategy=\"steps\",        # Evaluation strategy: \"steps\" or \"epoch\"\n",
        "    eval_steps=100,                     # Evaluate every 100 steps\n",
        "    logging_strategy=\"steps\",           # Logging strategy: \"steps\" or \"epoch\"\n",
        "    logging_steps=100,                  # Log every 100 steps\n",
        "    save_strategy=\"steps\",              # Saving strategy: \"steps\" or \"epoch\"\n",
        "    save_steps=100,                     # Save every 100 steps\n",
        "    save_total_limit=3,                 # Save only the last 3 checkpoints at any given time during training\n",
        "    metric_for_best_model='wups',       # Metric used for determining the best model\n",
        "    per_device_train_batch_size=32,     # Batch size per GPU for training\n",
        "    per_device_eval_batch_size=32,      # Batch size per GPU for evaluation\n",
        "    remove_unused_columns=False,        # Whether to remove unused columns in the dataset\n",
        "    num_train_epochs=5,                 # Number of training epochs\n",
        "    fp16=True,                          # Enable mixed precision training (float16)\n",
        "    dataloader_num_workers=8,           # Number of workers for data loading\n",
        "    load_best_model_at_end=True,        # Whether to load the best model at the end of training\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:54:59.026061Z",
          "iopub.execute_input": "2023-11-11T19:54:59.026493Z",
          "iopub.status.idle": "2023-11-11T19:54:59.03876Z",
          "shell.execute_reply.started": "2023-11-11T19:54:59.026459Z",
          "shell.execute_reply": "2023-11-11T19:54:59.037698Z"
        },
        "trusted": true,
        "id": "46Ia3enS0w18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "wandb_key = user_secrets.get_secret(\"wandb2\")\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "wandb.login(key = wandb_key)\n",
        "!wandb status"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:55:18.476249Z",
          "iopub.execute_input": "2023-11-11T19:55:18.47667Z",
          "iopub.status.idle": "2023-11-11T19:55:24.835538Z",
          "shell.execute_reply.started": "2023-11-11T19:55:18.476637Z",
          "shell.execute_reply": "2023-11-11T19:55:24.834133Z"
        },
        "trusted": true,
        "id": "Jlc5KT0T0w18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_train_model(dataset, args, text_model='bert-base-uncased', image_model='google/vit-base-patch16-224-in21k', multimodal_model='bert_vit'):\n",
        "    \"\"\"\n",
        "    Creates a Multimodal VQA collator and model, and trains the model using the provided dataset and training arguments.\n",
        "\n",
        "    Args:\n",
        "        dataset (Dict): Dictionary containing 'train' and 'test' datasets.\n",
        "        args (TrainingArguments): Training arguments for the model.\n",
        "        text_model (str): Pretrained name for the text encoder.\n",
        "        image_model (str): Pretrained name for the image encoder.\n",
        "        multimodal_model (str): Name for the multimodal model.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: Collator, model, training metrics, and evaluation metrics.\n",
        "    \"\"\"\n",
        "    # Create Multimodal Collator and Model\n",
        "    collator, model = create_multimodal_vqa_collator_and_model(text_model, image_model)\n",
        "\n",
        "    # Create a copy of arguments and set the output directory\n",
        "    multi_args = deepcopy(args)\n",
        "    multi_args.output_dir = os.path.join(\"/kaggle/working/checkpoint/\", multimodal_model)\n",
        "    print(multi_args.output_dir)\n",
        "    # Create Trainer for Multimodal Model\n",
        "    multi_trainer = Trainer(\n",
        "        model,\n",
        "        multi_args,\n",
        "        train_dataset=dataset['train'],\n",
        "        eval_dataset=dataset['test'],\n",
        "        data_collator=collator,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    # Train and evaluate for metrics\n",
        "    train_multi_metrics = multi_trainer.train()\n",
        "    eval_multi_metrics = multi_trainer.evaluate()\n",
        "\n",
        "    return collator, model, train_multi_metrics, eval_multi_metrics, multi_trainer\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:55:33.487528Z",
          "iopub.execute_input": "2023-11-11T19:55:33.488964Z",
          "iopub.status.idle": "2023-11-11T19:55:33.498284Z",
          "shell.execute_reply.started": "2023-11-11T19:55:33.488919Z",
          "shell.execute_reply": "2023-11-11T19:55:33.497208Z"
        },
        "trusted": true,
        "id": "n8Zw8OTZ0w19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collator, model, train_multi_metrics, eval_multi_metrics, trainer = create_and_train_model(dataset, args)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T19:55:34.363266Z",
          "iopub.execute_input": "2023-11-11T19:55:34.3643Z",
          "iopub.status.idle": "2023-11-11T20:22:58.898048Z",
          "shell.execute_reply.started": "2023-11-11T19:55:34.364261Z",
          "shell.execute_reply": "2023-11-11T20:22:58.89686Z"
        },
        "trusted": true,
        "id": "lkkZXDb70w19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_multi_metrics"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T20:22:58.900571Z",
          "iopub.execute_input": "2023-11-11T20:22:58.90091Z",
          "iopub.status.idle": "2023-11-11T20:22:58.9088Z",
          "shell.execute_reply.started": "2023-11-11T20:22:58.900878Z",
          "shell.execute_reply": "2023-11-11T20:22:58.90775Z"
        },
        "trusted": true,
        "id": "q7tQ4wM10w19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_folder = \"/kaggle/working/checkpoint/bert_vit/\"\n",
        "\n",
        "\n",
        "# Get a list of all subdirectories in the checkpoint folder\n",
        "all_subdirectories = [d for d in os.listdir(checkpoint_folder) if os.path.isdir(os.path.join(checkpoint_folder, d))]\n",
        "\n",
        "# Filter only subdirectories starting with \"checkpoint-\"\n",
        "checkpoint_subdirectories = [d for d in all_subdirectories if d.startswith(\"checkpoint-\")]\n",
        "\n",
        "# Extract the checkpoint numbers from the subdirectory names\n",
        "checkpoint_numbers = [int(d.split(\"-\")[1]) for d in checkpoint_subdirectories]\n",
        "\n",
        "# Find the latest checkpoint number\n",
        "latest_checkpoint_number = max(checkpoint_numbers, default=0)\n",
        "\n",
        "# Construct the path for the latest checkpoint\n",
        "latest_checkpoint_path = os.path.join(checkpoint_folder, f\"checkpoint-{latest_checkpoint_number}/pytorch_model.bin\")\n",
        "\n",
        "print(\"Latest Checkpoint Number:\", latest_checkpoint_number)\n",
        "print(\"Latest Checkpoint Path:\", latest_checkpoint_path)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T20:22:58.910144Z",
          "iopub.execute_input": "2023-11-11T20:22:58.910447Z",
          "iopub.status.idle": "2023-11-11T20:22:58.923907Z",
          "shell.execute_reply.started": "2023-11-11T20:22:58.910422Z",
          "shell.execute_reply": "2023-11-11T20:22:58.922729Z"
        },
        "trusted": true,
        "id": "6OitrfJB0w19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalVQAModel()\n",
        "model.load_state_dict(torch.load(latest_checkpoint_path))\n",
        "model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T20:22:58.926656Z",
          "iopub.execute_input": "2023-11-11T20:22:58.926984Z",
          "iopub.status.idle": "2023-11-11T20:23:06.990569Z",
          "shell.execute_reply.started": "2023-11-11T20:22:58.926958Z",
          "shell.execute_reply": "2023-11-11T20:23:06.989124Z"
        },
        "trusted": true,
        "id": "yzDpmgyx0w19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Randomly sample 10 indices from the test dataset\n",
        "random_sample_indices = random.sample(range(len(dataset[\"test\"])), k=10)\n",
        "\n",
        "# Sample data for manual testing\n",
        "sample = collator([dataset[\"test\"][index] for index in random_sample_indices])\n",
        "\n",
        "# Extract input components from the sample for manual testing\n",
        "input_ids = sample[\"input_ids\"].to(device)\n",
        "token_type_ids = sample[\"token_type_ids\"].to(device)\n",
        "attention_mask = sample[\"attention_mask\"].to(device)\n",
        "pixel_values = sample[\"pixel_values\"].to(device)\n",
        "labels = sample[\"labels\"].to(device)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T20:23:06.99204Z",
          "iopub.execute_input": "2023-11-11T20:23:06.992491Z",
          "iopub.status.idle": "2023-11-11T20:23:07.159179Z",
          "shell.execute_reply.started": "2023-11-11T20:23:06.992452Z",
          "shell.execute_reply": "2023-11-11T20:23:07.157983Z"
        },
        "trusted": true,
        "id": "1B8U0w4i0w19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Forward pass with the sample data\n",
        "output = model(input_ids, pixel_values, attention_mask, token_type_ids, labels)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T20:23:07.160982Z",
          "iopub.execute_input": "2023-11-11T20:23:07.161478Z",
          "iopub.status.idle": "2023-11-11T20:23:07.393351Z",
          "shell.execute_reply.started": "2023-11-11T20:23:07.161439Z",
          "shell.execute_reply": "2023-11-11T20:23:07.392175Z"
        },
        "trusted": true,
        "id": "0lzl8Wfk0w1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from the model output\n",
        "predictions = output[\"logits\"].argmax(axis=-1).cpu().numpy()\n",
        "predictions\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T20:23:07.395908Z",
          "iopub.execute_input": "2023-11-11T20:23:07.397014Z",
          "iopub.status.idle": "2023-11-11T20:23:07.446138Z",
          "shell.execute_reply.started": "2023-11-11T20:23:07.396973Z",
          "shell.execute_reply": "2023-11-11T20:23:07.445003Z"
        },
        "trusted": true,
        "id": "tqJncOV60w1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def similarity(a, b):\n",
        "    # Split words if it is a list and remove extra spaces\n",
        "    words_a = [w.strip() for w in a.split(',')]\n",
        "    words_b = [w.strip() for w in b.split(',')]\n",
        "\n",
        "    # Split words if connected by underscore _\n",
        "    a = [w_ for word in words_a for w_ in word.split('_')]\n",
        "    b = [w_ for word in words_b for w_ in word.split('_')]\n",
        "\n",
        "    res = 0\n",
        "    n = 0\n",
        "\n",
        "    # Calculate score and take average\n",
        "    for i in a:\n",
        "        synsets_i = wordnet.synsets(i)\n",
        "        if synsets_i:\n",
        "            s1 = synsets_i[0]\n",
        "            for j in b:\n",
        "                synsets_j = wordnet.synsets(j)\n",
        "                if synsets_j:\n",
        "                    s2 = synsets_j[0]\n",
        "                    sim = s1.wup_similarity(s2)\n",
        "                    if sim:\n",
        "                        res += sim\n",
        "                    n += 1\n",
        "\n",
        "    return res / n if n != 0 else 0\n",
        "\n",
        "# Show predictions for a range of examples\n",
        "for i in range(2000, 2005):\n",
        "    print(\"\\n=========================================================\\n\")\n",
        "    real_answer = show_example(train=False, idx=i)\n",
        "    predicted_answer = answer_space[preds[i - 2000]]\n",
        "    print(\"Predicted Answer:\\t\", predicted_answer)\n",
        "    print(f\"Similarity: {similarity(real_answer, predicted_answer)}\")\n",
        "    print(\"\\n=========================================================\\n\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T20:23:07.447773Z",
          "iopub.execute_input": "2023-11-11T20:23:07.448823Z",
          "iopub.status.idle": "2023-11-11T20:23:08.868791Z",
          "shell.execute_reply.started": "2023-11-11T20:23:07.448788Z",
          "shell.execute_reply": "2023-11-11T20:23:08.867297Z"
        },
        "trusted": true,
        "id": "mTSCwUcB0w1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Count and print the number of trainable parameters in a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The PyTorch model.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(\"Number of trainable parameters: {:,}\".format(num_params))\n",
        "count_trainable_parameters(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-11T20:23:08.87008Z",
          "iopub.execute_input": "2023-11-11T20:23:08.870414Z",
          "iopub.status.idle": "2023-11-11T20:23:08.880645Z",
          "shell.execute_reply.started": "2023-11-11T20:23:08.870387Z",
          "shell.execute_reply": "2023-11-11T20:23:08.879418Z"
        },
        "trusted": true,
        "id": "jCjrEK6G0w1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OTnumFt70w1-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}